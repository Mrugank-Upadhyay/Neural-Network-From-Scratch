# Neural Networks from Scratch

In this project, instead of utilizing frameworks such as tensorflow to create a neural network, I will be creating one using only Python and Numpy.

This is mainly to get into the nitty-gritty details of how NNs work by learning the different activation functions, optimizers, loss functions, the specifics behind training and testing data, different methods to stop overfitting and underfitting, and working with datasets (data preparation, loading, preprocessing, shuffling). In doing so, I will explore the usecases, advantages, and disadvantages of each to improve my understanding of NNs.

## What I will implement

- Activation Functions: Sigmoid, ReLU
- Loss Function: Binary Cross Entropy, Categorical Cross Entropy, Mean Squared Error, Mean Absolute Error
- Optimizers: SGD, AdaGrad, RMSprop, Adam
- Backpropogation
- Overfitting / Underfitting: L1 & L2 regularization, dropout
- Binary Logistic Regression
- Hyperparameter Tuning
    - I probably will try to explore different automated methods of hyperparameter tuning such as grid search and random search.